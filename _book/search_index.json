[
["index.html", "Causal Inference the Mixtape R-mixed 1 Introduction 1.1 Why I wrote this 1.2 What you need to follow along", " Causal Inference the Mixtape R-mixed Alex Stephenson 2020-02-27 1 Introduction Scott Cunningham’s Causal Inference the Mixtape is one of the best books that I have read on the practical application of methods in ages. I am excited for the final version, and appreciate all the effort that he has put into making the book work. In my view, the book has only one minor flaw. Stata. 1.1 Why I wrote this As a consequence, I wrote this short translation in order to show how to do all of the examples in Mixtape in R for two major reasons. First, Stata is not free software, in either the beer or the speech sense. This means that any person trying to pick up material and see example code will be out of luck if their only examples are Stata do files. R is a better choice (for me) because R is both free software and R programs look an awful lot like programs written in Python, Julia, etc, which is not at all true for Stata. As a consequence, I find that R travels better when translating concepts between languages. Second, Causal Inference the Mixtape when finally published will have an R component, but it does not right now in the current version on Scott Cunningham’s website. The book is too useful to be limited to those who only know Stata. 1.2 What you need to follow along You need a new copy of R. You also need to run the following line of code. This line of code will install the tidyverse series of packages and the estimatr package. From the tidyverse, I primarily make use of ggplot2, haven for reading in .dta files, and dplyr for data manipulation. estimatr allows us to run robust regressions with standard errors that mimic Stata. install.packages(c(&quot;tidyverse&quot;,&quot;ggpubr&quot;,&quot;estimatr&quot;)) "],
["linear-regression.html", "2 Linear Regression 2.1 An Example 2.2 Regression Plot 2.3 Residual Plot 2.4 OLS Residuals Add up to Zero by Construction 2.5 Monte Carlo Simulation", " 2 Linear Regression 2.1 An Example Note: the output will not be identical because STATA’s RNG is unclear, but the coefficients will be close set.seed(1) N &lt;- 10000 x &lt;- rnorm(N, 0,1) u &lt;- rnorm(N, 0,1) y &lt;- 5.5*x + 12*u m1d &lt;- data.frame(y = y, x=x, u =u) m1 &lt;- lm(y~x, data = m1d) yhat1 &lt;- predict(m1) yhat2 &lt;- 0.0732608 + 5.685033*x uhat1 &lt;- residuals(m1) uhat2 &lt;- y - yhat2 # Bring all of them into a data frame for plotting m1d &lt;- data.frame(y = y, x = x, u = u, uhat1 = uhat1, uhat2 = uhat2, yhat1 = yhat1) 2.2 Regression Plot library(ggplot2) theme_set(theme_bw()) ggplot(data = m1d, aes(x = x, y = y))+ geom_point(size = .1)+ geom_smooth(method = &quot;lm&quot;, se =F)+ geom_hline(yintercept = 0, linetype = &quot;dashed&quot;)+ geom_vline(xintercept = 0, linetype = &quot;dashed&quot;)+ ggtitle(&quot;OLS Regression Line&quot;) 2.3 Residual Plot To make a residual plot, we can exploit a little trick in ggplot. Another way to do this would be to manually specify variables in the aes() function as in the previous example. ggplot(lm(y~x, data = m1d))+ geom_point(aes(x = .fitted, y = .resid), color = &quot;gray&quot;)+ geom_hline(yintercept = 0, col = &quot;black&quot;)+ xlab(&quot;Fitted Values&quot;)+ ylab(&quot;Residuals&quot;)+ # Remove the legend because it is unnecessary theme(legend.position = &quot;none&quot;)+ ggtitle(&quot;Residual Plot&quot;) 2.4 OLS Residuals Add up to Zero by Construction # Brand new set of variables set.seed(1234) N &lt;- 10 x &lt;- 9*rnorm(N) u &lt;- 36*rnorm(N) y = 3 + 2*x + u m2 &lt;- lm(y~x) Here is a different way to get the values of a model. We’re directly calling attributes of our lm object fitted &lt;- m2$fitted.values yhat&lt;- predict(m2) residuals &lt;- m2$residuals print(cbind(x,u,y, yhat,residuals)) ## x u y yhat residuals ## 1 -10.863592 -17.178937 -35.906121 -17.4087418 -18.497379 ## 2 2.496863 -35.941912 -27.948186 -0.7283636 -27.219822 ## 3 9.759971 -27.945140 -5.425199 8.3395447 -13.764744 ## 4 -21.111279 2.320517 -36.902041 -30.2028650 -6.699176 ## 5 3.862122 34.541786 45.266031 0.9761470 44.289884 ## 6 4.554503 -3.970278 8.138728 1.8405767 6.298152 ## 7 -5.172660 -18.396342 -25.741661 -10.3036769 -15.437985 ## 8 -4.919687 -32.803035 -39.642408 -9.9878430 -29.654565 ## 9 -5.080068 -30.138180 -37.298316 -10.1880772 -27.110239 ## 10 -8.010340 86.970066 73.949386 -13.8464896 87.795875 print(round(sum(residuals),8)) ## [1] 0 2.5 Monte Carlo Simulation set.seed(1234) N &lt;- 10000 sim_ols &lt;- function(N){ x &lt;- 9*rnorm(N) u &lt;- 36*rnorm(N) y = 3+2*x + u model &lt;- lm(y~x) # Since there are only two coefficients the second is $\\Beta_1$ beta &lt;- model$coefficients[2] } # Preallocate memory to our list to speed computation beta &lt;- c(NA_real_) length(beta) &lt;- 1000 # For loops are fine for intuition, and the appreciable loss of speed in negligible for most applications for(i in 1:1000){ beta[i]&lt;-sim_ols(N) } mean(beta) ## [1] 2.000071 ggplot(data = data.frame(beta = beta), aes(x=beta))+ geom_histogram(bins = 30)+ geom_vline(xintercept = mean(beta))+ xlab(&quot;Beta Estimate&quot;)+ ylab(&quot;Number of Observations&quot;)+ ggtitle(&quot;Monte Carlo Simulation for Treatment Effect&quot;) "],
["dags.html", "3 DAGs 3.1 Collider Bias 3.2 Collider Bias 2 Qualitative Change in Sign 3.3 Collider Bias Nonrandom Sample selection", " 3 DAGs 3.1 Collider Bias set.seed(541) N &lt;- 10000 female &lt;- runif(N) &gt;=0.5 ability &lt;- rnorm(N) discrimination &lt;- female occupation &lt;- 1 + 2*ability + 0*female - 2*discrimination + rnorm(N) wage &lt;- 1 - discrimination + occupation + 2*ability + rnorm(N) # Create collider Bias lm(wage ~ female) ## ## Call: ## lm(formula = wage ~ female) ## ## Coefficients: ## (Intercept) femaleTRUE ## 1.944 -2.858 lm(wage~female + occupation) ## ## Call: ## lm(formula = wage ~ female + occupation) ## ## Coefficients: ## (Intercept) femaleTRUE occupation ## 0.1822 0.6382 1.7921 lm(wage~female + occupation + ability) ## ## Call: ## lm(formula = wage ~ female + occupation + ability) ## ## Coefficients: ## (Intercept) femaleTRUE occupation ability ## 0.9863 -0.9716 0.9954 2.0080 3.2 Collider Bias 2 Qualitative Change in Sign library(estimatr) set.seed(541) N &lt;- 2500 z &lt;- rnorm(N) k &lt;- rnorm(N, 10, 4) d &lt;- ifelse(k &gt;=12, 1, 0) # Treatment effect is 50. Notice that y is not a function of X y &lt;- 50*d + 100 + rnorm(N) x &lt;- 50*d + y + rnorm(N, 50, 1) collider2 &lt;- data.frame(y =y, x= x, d = d) lm_robust(y~d, data = collider2, se_type = &quot;stata&quot;) ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF ## (Intercept) 99.98252 0.02405992 4155.564 0 99.93534 100.02970 2498 ## d 50.01345 0.04229452 1182.504 0 49.93051 50.09638 2498 lm_robust(y~x, data = collider2, se_type = &quot;stata&quot;) ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper ## (Intercept) 24.9846086 0.056062156 445.6591 0 24.8746755 25.0945416 ## x 0.4999951 0.000297306 1681.7525 0 0.4994121 0.5005781 ## DF ## (Intercept) 2498 ## x 2498 lm_robust(y~d+x, data = collider2, se_type = &quot;stata&quot;) ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower ## (Intercept) 26.1083212 1.468030563 17.7845897 1.020515e-66 23.2296388 ## d 0.7501925 0.981099670 0.7646445 4.445555e-01 -1.1736601 ## x 0.4925021 0.009794007 50.2860663 0.000000e+00 0.4732969 ## CI Upper DF ## (Intercept) 28.9870036 2497 ## d 2.6740450 2497 ## x 0.5117073 2497 3.3 Collider Bias Nonrandom Sample selection library(ggplot2) library(ggpubr) set.seed(3444) N &lt;- 2500 beauty &lt;- rnorm(N) talent &lt;- rnorm(N) # Collider variable score &lt;- beauty + talent cutoff &lt;- as.numeric(quantile(score, probs = .85)) star &lt;- ifelse(score &gt;= cutoff,1, 0) movie &lt;- data.frame(beauty = beauty, talent = talent, star = star ) overall &lt;- ggplot(movie, aes(beauty, talent))+ geom_point() splitByStar &lt;- ggplot(movie, aes(beauty, talent))+ geom_point()+ facet_wrap(~star) ggarrange(splitByStar, overall, ncol = 1, nrow = 2) "],
["potential-outcomes.html", "4 Potential Outcomes 4.1 Yule Regression 4.2 Monte Carlo Simulation of the SDO 4.3 Krueger Replication", " 4 Potential Outcomes library(estimatr) library(ggplot2) library(haven) 4.1 Yule Regression # Yule dataset goes here yule &lt;- read_dta(&quot;https://storage.googleapis.com/causal-inference-mixtape.appspot.com/yule.dta&quot;) # Yule&#39;s regression lm_robust(paup~outrelief + old + pop, data = yule, se_type = &quot;stata&quot;) ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower ## (Intercept) 63.1877372 42.71255368 1.4793716 0.1502048458 -24.3049628 ## outrelief 0.7520945 0.16798829 4.4770650 0.0001156568 0.4079861 ## old 0.0556020 0.36803485 0.1510781 0.8809973217 -0.6982832 ## pop -0.3107383 0.07440422 -4.1763536 0.0002614333 -0.4631485 ## CI Upper DF ## (Intercept) 150.6804372 28 ## outrelief 1.0962029 28 ## old 0.8094872 28 ## pop -0.1583282 28 4.2 Monte Carlo Simulation of the SDO set.seed(1) # Monte Carlo Simulation sdo &lt;- function(N) { # Generate potential outcomes # By construction of these potential outcomes # The true ATE is 0.6 y1 &lt;- c(7, 5, 5, 7, 4, 10, 1, 5, 3, 9) y0 &lt;- c(1, 6, 1, 8, 2, 1, 10, 6, 7, 8) # Generate treatment vector d &lt;- sample(c(rep(1, N/2), rep(0, N/2)), N, replace = F) # Generate observed y y &lt;- d * y1 + (1 - d) * y0 # Get the SDO of this assignment sdo &lt;- mean(y[d == 1]) - mean(y[d == 0]) return(sdo) } # Preallocate memory to our list to speed computation mc_result &lt;- c(NA_real_) length(mc_result) &lt;- 10000 # Simulation 10000 draws for(i in 1:10000){ mc_result[i] &lt;- sdo(10) } # This returns our best guess of the result via simulation mean(mc_result) ## [1] 0.59288 ggplot(data = data.frame(x = mc_result), aes(x=x))+ geom_histogram(bins = 20)+ # The true ATE geom_vline(xintercept = 0.6)+ xlab(&quot;Estimate&quot;)+ ylab(&quot;Number of Times Appearing in Simulation&quot;)+ ggtitle(&quot;SDO Simulation&quot;) 4.3 Krueger Replication # Krueger&#39;s Star Dataset star_sw &lt;- read_dta(&quot;https://storage.googleapis.com/causal-inference-mixtape.appspot.com/star_sw.dta&quot;) # Regression 1 lm_robust(tscorek ~ sck + rak, data = star_sw, se_type = &quot;stata&quot;) ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower ## (Intercept) 918.0428928 1.633391 562.0472691 0.000000e+00 914.840835 ## sck 13.8989945 2.454093 5.6635966 1.553732e-08 9.088053 ## rak 0.3139354 2.270976 0.1382381 8.900570e-01 -4.138027 ## CI Upper DF ## (Intercept) 921.244950 5783 ## sck 18.709936 5783 ## rak 4.765898 5783 "],
["matching.html", "5 Matching 5.1 Titanic Data", " 5 Matching library(haven) library(ggplot2) library(dplyr) 5.1 Titanic Data titanic &lt;- read_dta(&quot;https://storage.googleapis.com/causal-inference-mixtape.appspot.com/titanic.dta&quot;) # Generate sample equivalence titanic_ua &lt;- titanic %&gt;% mutate(female = ifelse(sex == 0, 1, 0), male = ifelse(sex == 1, 1, 0), s = case_when( female == 1 &amp; age == 1~1, female == 1 &amp; age == 0~2, female == 0 &amp; age == 1~3, female == 0 &amp; age == 0~4, ), d = ifelse(class == 1, 1, 0)) diff &lt;- round((mean(titanic_ua$survived[titanic_ua$d == 1])-mean(titanic_ua$survived[titanic_ua$d == 0]))*100, 1) print(paste0(&quot;SDO says that being in first class\\n raised the probability of survival by &quot;, diff, &quot;%&quot;)) ## [1] &quot;SDO says that being in first class\\n raised the probability of survival by 35.4%&quot; # Adjust for age and gender "]
]
